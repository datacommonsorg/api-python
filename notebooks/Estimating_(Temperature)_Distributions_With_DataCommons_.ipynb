{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XQfQekexygxn",
        "oXfNxahXzCXH",
        "xySpvIoAYT1m",
        "BX_FwVZ82DV5",
        "1VEx6Zm84-0W"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preamble\n",
        "**Context**\n",
        "\n",
        "   The climate at a specific location (as recorded by observations) can be seen as a sample from a \"real\" probability distribution function (PDF). We have a number of climate models, from many agencies across the world, that use physics to approximate this underlying PDF. Each simulation run of each of these models can be seen as a sample from the PDF embodied by the code in that model.\n",
        "\n",
        "   Some agencies publish the results of multiple simulation runs for their models while others publish a single set of predictions (from either a single simulation or an aggregation of multiple simulations).\n",
        "\n",
        "   Each model defines a set of polygons covering the earth. Results are typically hourly predictions (over the next several decades, starting from 10-20 years ago), for a number of variables (such as the maximum and minimum temperatures, precipitation) for each of the polygons. The polygons are typically 100km x 100km or 0.5arc degree S2 cells. Combining results from different models is complicated by the differences in these geometries. Further, the large size of these polygons means that it is difficult to make predictions about most cities and counties, which are much smaller. Fortunately, NASA produces downscaled, normalized versions of the outputs of the most well regarded models. We use these downscaled models for our analysis.\n",
        "\n",
        "   Models also tend to have a bias, i.e., they either over or under estimate the temperature. Since the models typically start their prediction at some time in the past (e.g., 2005), the differences between observed and predicted values can be used to estimate and compensate for these biases. The NASA downscaled data incorporates this bias correction.\n",
        "\n",
        "   We would like to answer the following class of questions: given a place (typically an administrative area), what is the highest / lowest temperature that is likely to be reached, for some period of time (e.g., one day), during some period of time (e.g., 2024 or 2030-2040) with X% likelihood (for X=1% and X=5%). To answer these questions, we first reconstruct the PDF of the variable (highest or lowest temperature) in question and then answer the question by sampling from this PDF.\n",
        "\n",
        "   Consider the case of estimating the highest temperature $T$ on a single day, in a single given year, with a likelihood of $X$%. To do this, we\n",
        "   1.  construct the PDF of daily highest temperature.\n",
        "   2.  pick a $T$. Let the cumulative probability of the highest temperature being less than $T$ on a randomly chosen day be $p$.\n",
        "   3. We compute the probability of the temperature being less than $T$ on every day of that year. We iterate on $T$ till we find a $T$ where this probability is $1-X$.\n",
        "\n",
        "\n",
        "   **Constructing the PDF**\n",
        "\n",
        "   We have some number of models (see below for the list of models we use in this analysis) and for each we have a table of maximum temperature and date. From this, we construct the PDF. We try the following approaches.\n",
        "   1. We quantize the temperature into $1^oC$ ranges and build the histogram of temperatures vs the number of days where that is the highest temperature. The probability of the highest temperature being $T$ can be read off this histogram. This is a very conservative approach, where the temperatures are bound to within the range of predictions for that year. Given that the table from the model itself is just an estimate, the actual highest temperature could indeed be higher than what is in the table. The next approach tries to do this.\n",
        "   2. We can fit a distribution to the points in the table. We experiment with Gaussian, Gamma, and mixture of Gaussians. We notice that the problem with simple Gaussian and Gamma is that while they do predict much higher temperatures, they are not aware of physical limits (e.g., they assign a non-zero probability to the temperature exceeding $100^oC$). So, as the time period (over which the prediction is done) gets longer, the probability of much more extreme events goes up, well into unrealistic territory. We notice that mixture of Gaussians does have more realistic behaviour.\n",
        "\n",
        "   The purpose of this exercise is to enable planning for extreme weather events. We notice that even with the most conservative histogram based approach, there are likely to be days, in the next decade, where the temperature exceeds $50^oC$, a dangerous threshold.\n",
        "\n",
        "   **Independence**\n",
        "\n",
        "   Let the cumulative probability of the temperature being less than $T$ on a randomly chosen day be $p$. If the temperatures on different days were indepdentent of each other, the probability of the peak temperature exceeding $T$ on at least one day in the year would simply be $(1 - p^{N})$, where $N=365$. However, the temperature on consecutive days are not independent of each other. In this analysis, we approximately capture this by using a lower $N$. For example, if we can assume that the temperatures on days 7 or more days apart are independent, we can use $N=52$."
      ],
      "metadata": {
        "id": "2mLV1WsYJ5PW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "XQfQekexygxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "w2JnB_1iymIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from IPython.display import display\n",
        "from ipywidgets import Box\n",
        "import ipywidgets as widgets\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "metadata": {
        "id": "yA9qvF2Vylao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_TEMP_C = -100\n",
        "MAX_TEMP_C = 120"
      ],
      "metadata": {
        "id": "476GscrIce7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Helper Functions"
      ],
      "metadata": {
        "id": "oXfNxahXzCXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_kelvins(val_c):\n",
        "  return val_c + 273.15\n",
        "\n",
        "def convert_to_celsius(val_k):\n",
        "  return val_k - 273.15"
      ],
      "metadata": {
        "id": "pV0wsQdIzEf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extracts Retrieval"
      ],
      "metadata": {
        "id": "xySpvIoAYT1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_extract(file_id):\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.FetchMetadata(fetch_all=True)\n",
        "  downloaded.GetContentFile(downloaded.metadata['title'])\n",
        "  return pd.read_csv(downloaded.metadata['title'], keep_default_na=False)"
      ],
      "metadata": {
        "id": "971ZLMqaYWc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Probabilities - Helpers"
      ],
      "metadata": {
        "id": "BX_FwVZ82DV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gmm_choose_best_num_mixture(obs):\n",
        "  opt_bic = None\n",
        "  min_bic = 0\n",
        "  counter=1\n",
        "  for i in range (1, 5, 1): # test the AIC/BIC metric between 1 and 10 components\n",
        "    gmm = GMM(n_components = i, max_iter=1000, random_state=0, covariance_type = 'full')\n",
        "    gmm.fit(obs).predict(obs)\n",
        "    bic = gmm.bic(obs)\n",
        "    if bic < min_bic or min_bic == 0:\n",
        "      min_bic = bic\n",
        "      opt_bic = i\n",
        "\n",
        "  return opt_bic"
      ],
      "metadata": {
        "id": "ElsydGcc2o-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_dist(df_subset, dist_type):\n",
        "  obs_list = df_subset[\"value\"].tolist()\n",
        "  params = []\n",
        "\n",
        "  if dist_type == \"Gamma\":\n",
        "    params = stats.gamma.fit(obs_list)\n",
        "  elif dist_type == \"Gaussian\":\n",
        "    params  = stats.norm.fit(obs_list)\n",
        "  elif dist_type == \"GaussianMixture\":\n",
        "    obs= np.array(obs_list)\n",
        "    obs = obs.reshape(-1, 1)\n",
        "    best_num_mixture = gmm_choose_best_num_mixture(obs)\n",
        "    # print(f\"Best GMM mixture has {best_num_mixture} components.\")\n",
        "    gmm = GMM(n_components = best_num_mixture, max_iter=2000, random_state=0, covariance_type = 'full')\n",
        "    params = gmm.fit(obs)\n",
        "  elif dist_type == \"Histogram\":\n",
        "    return []\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "1wLNOYh02IMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_smooth_pdf(x_min, x_max, dist_type, params):\n",
        "  y = []\n",
        "\n",
        "  bins = []\n",
        "  for b in range(x_min, x_max):\n",
        "    bins.append(b)\n",
        "\n",
        "  if dist_type == \"Gamma\":\n",
        "    shape = params[0]\n",
        "    loc = params[1]\n",
        "    scale = params[2]\n",
        "    y = stats.gamma.pdf(bins, shape, loc, scale)\n",
        "  elif dist_type == \"Gaussian\":\n",
        "    loc = params[0]\n",
        "    scale = params[1]\n",
        "    y = stats.norm.pdf(bins, loc, scale)\n",
        "  elif dist_type == \"GaussianMixture\":\n",
        "    mean = params.means_\n",
        "    covs = params.covariances_\n",
        "    weights = params.weights_\n",
        "\n",
        "    num_gaussians = mean.shape[0]\n",
        "    y = None\n",
        "    for i in range(num_gaussians):\n",
        "      y_i = stats.norm.pdf(bins, float(mean[i][0]), np.sqrt(float(covs[i][0][0])))*weights[i] # i-th gaussian\n",
        "      if y is None:\n",
        "        y = y_i\n",
        "      else:\n",
        "        y += y_i\n",
        "\n",
        "  elif dist_type == \"Histogram\":\n",
        "    return []\n",
        "\n",
        "  return y"
      ],
      "metadata": {
        "id": "-eAOkeM924bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cdf(x, dist_type, params):\n",
        "  if dist_type == \"Gamma\":\n",
        "    shape = params[0]\n",
        "    loc = params[1]\n",
        "    scale = params[2]\n",
        "    return stats.gamma.cdf(x, shape, loc, scale)\n",
        "  elif dist_type == \"Gaussian\":\n",
        "    loc = params[0]\n",
        "    scale = params[1]\n",
        "    return stats.norm.cdf(x, loc, scale)\n",
        "  elif dist_type == \"GaussianMixture\":\n",
        "    means = params.means_\n",
        "    covs = params.covariances_\n",
        "    weights = params.weights_\n",
        "\n",
        "    mcdf = 0.0\n",
        "    for i in range(len(weights)):\n",
        "        mcdf += weights[i] * stats.norm.cdf(x, loc=means[i][0], scale=np.sqrt(covs[i][0][0])) # i-th gaussian\n",
        "    return mcdf"
      ],
      "metadata": {
        "id": "05GS_9SS3yCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_histogram_cdf(df_hist_subset):\n",
        "  sum = 0\n",
        "  temp2pdf = dict(zip(df_hist_subset['temp'], df_hist_subset['prob']))\n",
        "  temp2cdf = {}\n",
        "  for temp in range(int(MIN_TEMP_C), int(MAX_TEMP_C)):\n",
        "    sum += temp2pdf.get(temp, 0)\n",
        "    temp2cdf[temp] = sum\n",
        "  return temp2cdf"
      ],
      "metadata": {
        "id": "AOaNRVgDS2Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def icdf_percent_n(dist_type, temp2cdf_hist, params, var=\"Max_Temperature\", percent=0.01, n=1):\n",
        "  low = MIN_TEMP_C\n",
        "  high = MAX_TEMP_C\n",
        "  mid = int((low + high) / 2.0)\n",
        "\n",
        "  is_max = True\n",
        "  if \"Min\" in var:\n",
        "    is_max = False\n",
        "\n",
        "  if dist_type == \"Histogram\":\n",
        "    curr_cdf = temp2cdf_hist[mid]\n",
        "  else:\n",
        "    curr_cdf = cdf(mid, dist_type, params)\n",
        "\n",
        "  if is_max:\n",
        "    icdf = 1.0 - (curr_cdf**n)\n",
        "  else:\n",
        "    icdf = 1.0 - (1.0 - curr_cdf)**n\n",
        "\n",
        "  iter = 0;\n",
        "  while (abs(icdf - percent) > 0.00025):\n",
        "    if iter > 500:\n",
        "      return mid\n",
        "    if (icdf > percent):\n",
        "      if is_max:\n",
        "        low = mid\n",
        "      else:\n",
        "        high = mid\n",
        "    else:\n",
        "      if is_max:\n",
        "        high = mid\n",
        "      else:\n",
        "        low = mid\n",
        "\n",
        "    mid = int((low + high) / 2.0)\n",
        "    if dist_type == \"Histogram\":\n",
        "      curr_cdf = temp2cdf_hist[mid]\n",
        "    else:\n",
        "      curr_cdf = cdf(mid, dist_type, params)\n",
        "\n",
        "    if is_max:\n",
        "      icdf = 1.0 - (curr_cdf**n)\n",
        "    else:\n",
        "      icdf = 1.0 - (1.0 - curr_cdf)**n\n",
        "\n",
        "    iter += 1\n",
        "  return mid"
      ],
      "metadata": {
        "id": "96AIFiQX4RGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Helpers"
      ],
      "metadata": {
        "id": "52bi_iHT83H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_fits(df_subset, df_hist, df_act, county_name, n, year_start, year_end, ssp, model, var):\n",
        "  dists = [\"NoFitting\", \"Histogram\", \"GaussianMixture\"] #[\"Histogram\", \"Gamma\", \"Gaussian\", \"GaussianMixture\"]\n",
        "\n",
        "  num_rows = len(dists)\n",
        "  num_cols = 1\n",
        "  fig, axs = plt.subplots(num_rows, num_cols, sharey=False, sharex=True, tight_layout=True)\n",
        "  fig.set_size_inches(10*num_cols, 4*num_rows, forward=True)\n",
        "\n",
        "  actual_val_col = \"tmax\"\n",
        "  is_max = True\n",
        "  if \"Min\" in var:\n",
        "    is_max = False\n",
        "    actual_val_col = \"tmin\"\n",
        "\n",
        "\n",
        "  x_min = int(df_subset[\"value\"].min()) - 5\n",
        "  x_max = int(df_subset[\"value\"].max()) + 20\n",
        "  x_array = list(range(x_min, x_max))\n",
        "  for i in range(len(dists)):\n",
        "    # Generate the bins for the data and show the histogram.\n",
        "\n",
        "    nbins = x_max - x_min\n",
        "    # If there is no actual data, only plot the CMIP6 model data.\n",
        "    count, bins, ignored = axs[i].hist(df_subset[\"value\"], nbins, density=True, color='lavender', label=\"CMIP6 Model Data Histogram\")\n",
        "    if not df_act.empty:\n",
        "      # If there is actual data, plot two histograms on top of each other. One histogram is represented as dots.\n",
        "      count_, bins_  = np.histogram(df_act[actual_val_col], nbins, density=True)\n",
        "      axs[i].scatter(bins_[:-1], count_, s=5, color='gray', label=\"Actual Data Histogram (dots)\")\n",
        "\n",
        "    no_fitting = False\n",
        "    dist = dists[i]\n",
        "    if dist == \"NoFitting\":\n",
        "      # When \"NoFitting\" is chosen, this just means N = 1 for the \"histogram fit\".\n",
        "      dist = \"Histogram\"\n",
        "      no_fitting = True\n",
        "\n",
        "    params = fit_dist(df_subset, dist)\n",
        "\n",
        "    # No smoothed_pdf produced for the Histogram fit.\n",
        "    smoothed_pdf_y = generate_smooth_pdf(x_min, x_max, dist, params)\n",
        "    if len(smoothed_pdf_y):\n",
        "      axs[i].plot(x_array, smoothed_pdf_y, linewidth=3, color='r', label=f\"{dist} \\nFitted (to CMIP6 Model Data)\")\n",
        "\n",
        "    # Get the 1% and 5% thresholds.\n",
        "    temp2cdf_hist = compute_histogram_cdf(df_hist)\n",
        "\n",
        "    if no_fitting:\n",
        "      n_to_use = 1\n",
        "    else:\n",
        "      n_to_use = n\n",
        "\n",
        "    p01 = icdf_percent_n(dist, temp2cdf_hist, params, var=var, percent=0.01, n=n_to_use)\n",
        "    p05 = icdf_percent_n(dist, temp2cdf_hist, params, var=var, percent=0.05, n=n_to_use)\n",
        "    p50 = icdf_percent_n(dist, temp2cdf_hist, params, var=var, percent=0.5, n=n_to_use)\n",
        "    p95 = icdf_percent_n(dist, temp2cdf_hist, params, var=var, percent=0.95, n=n_to_use)\n",
        "\n",
        "    # Add the p01 and p05 lines on the axes.\n",
        "    axs[i].axvline(p95, linestyle='--', color='limegreen', linewidth=2, label=\"95% threshold = {0}C\".format(p95))\n",
        "    axs[i].axvline(p50, linestyle='--', color='blue',  linewidth=2, label=\"50% threshold = {0}C\".format(p50))\n",
        "    axs[i].axvline(p05, linestyle='--', color='darkorange', linewidth=2, label=\"5% threshold = {0}C\".format(p05))\n",
        "    axs[i].axvline(p01, linestyle='--', color='crimson',  linewidth=2, label=\"1% threshold = {0}C\".format(p01))\n",
        "\n",
        "\n",
        "    # Add labels and title to the plot\n",
        "    if is_max:\n",
        "      axs[i].set_xlabel('Max Temperature (in C)')\n",
        "    else:\n",
        "      axs[i].set_xlabel('Min Temperature (in C)')\n",
        "    axs[i].set_ylabel('Probability Density')\n",
        "\n",
        "    if no_fitting:\n",
        "      axs[i].set_title(f'{model} ({ssp}). No Fitting (N= {n_to_use}). Period: ({year_start}-{year_end}). {county_name}')\n",
        "    else:\n",
        "      axs[i].set_title(f'{model} ({ssp}). Fit: {dist} (N= {n_to_use}). Period: ({year_start}-{year_end}). {county_name}')\n",
        "    if is_max:\n",
        "      axs[i].legend(loc=\"upper left\")\n",
        "    else:\n",
        "      axs[i].legend(loc=\"upper right\")\n",
        "\n",
        "    axs[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
        "\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "yh4vrJuQ85Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_trends(data_p1d, data_hist, county_dcid, ssp, model, var, n):\n",
        "  dists = [\"NoFitting\", \"Histogram\", \"GaussianMixture\"] #[\"Histogram\", \"Gamma\", \"Gaussian\", \"GaussianMixture\"]\n",
        "\n",
        "  year_starts = [1980, 1990, 2000, 2020, 2030, 2040]\n",
        "  colors_y = ['cyan', 'green', 'blue', 'gold', 'orange', 'red']\n",
        "\n",
        "  num_rows = len(dists) + 1\n",
        "  num_cols = 1\n",
        "  fig, axs = plt.subplots(num_rows, num_cols, sharey=False, sharex=False, tight_layout=True)\n",
        "  fig.set_size_inches(8*num_cols, 3*num_rows, forward=True)\n",
        "\n",
        "\n",
        "  is_max = True\n",
        "  if \"Min\" in var:\n",
        "    is_max = False\n",
        "\n",
        "  df_hist_ext = data_hist[var]\n",
        "  df_future_ext = data_p1d[(var, \"future\")]\n",
        "  df_past_ext = data_p1d[(var, \"past\")]\n",
        "\n",
        "  # Filters.\n",
        "  df_future_ext = df_future_ext[(df_future_ext['scenario'] == ssp) &\n",
        "                                (df_future_ext['county'] == county_dcid)]\n",
        "  df_past_ext = df_past_ext[(df_past_ext['scenario'] == \"historical\") &\n",
        "                              (df_past_ext['county'] == county_dcid)]\n",
        "\n",
        "  if Model != 'Ensemble':\n",
        "    df_future_ext = df_future_ext[df_future_ext[\"cmip6_model\"] == model]\n",
        "    df_past_ext = df_past_ext[df_past_ext[\"cmip6_model\"] == model]\n",
        "\n",
        "  df_hist_past_ext = df_hist_ext[(df_hist_ext['scenario'] == '') &\n",
        "                            (df_hist_ext['county'] == county_dcid) &\n",
        "                             (df_hist_ext['cmip6_model'] == model)]\n",
        "  df_hist_future_ext = df_hist_ext[(df_hist_ext['scenario'] == ssp) &\n",
        "                            (df_hist_ext['county'] == county_dcid) &\n",
        "                             (df_hist_ext['cmip6_model'] == model)]\n",
        "\n",
        "\n",
        "  x_min = min(int(df_future_ext[\"value\"].min()) - 5, int(df_past_ext[\"value\"].min()) - 5)\n",
        "  x_max = max(int(df_future_ext[\"value\"].max()) + 20, int(df_past_ext[\"value\"].max()) + 20)\n",
        "  x_array = list(range(x_min, x_max))\n",
        "  for i in range(len(dists)):\n",
        "\n",
        "    no_fitting = False\n",
        "    dist = dists[i]\n",
        "    if dist == \"NoFitting\":\n",
        "      dist = \"Histogram\"\n",
        "      no_fitting = True\n",
        "\n",
        "    p95s = []\n",
        "    p50s = []\n",
        "    p05s = []\n",
        "    p01s = []\n",
        "    fit_params = []\n",
        "    for y in year_starts:\n",
        "      y_end = y + 10\n",
        "\n",
        "      # We have two cases: past (prior to 2010) and future (after 2010).\n",
        "      if y < 2010:\n",
        "        df_hist_y = df_hist_past_ext[(df_hist_past_ext[\"year\"] >= y) & (df_hist_past_ext[\"year\"] < y_end)]\n",
        "        df_subset_y = df_past_ext[(df_past_ext[\"year\"] >= y) & (df_past_ext[\"year\"] < y_end)]\n",
        "      else:\n",
        "        df_hist_y = df_hist_future_ext[(df_hist_future_ext[\"year\"] >= y) & (df_hist_future_ext[\"year\"] < y_end)]\n",
        "        df_subset_y = df_future_ext[(df_future_ext[\"year\"] >= y) & (df_future_ext[\"year\"] < y_end)]\n",
        "\n",
        "      if df_hist_y.empty or df_subset_y.empty:\n",
        "        print(\"No data matched this selection. Please try a different selection.\")\n",
        "        return\n",
        "\n",
        "      temp2cdf_hist = compute_histogram_cdf(df_hist_y)\n",
        "\n",
        "      params = fit_dist(df_subset_y, dist)\n",
        "      fit_params.append(params)\n",
        "\n",
        "      if no_fitting:\n",
        "        n_to_use = 1\n",
        "      else:\n",
        "        n_to_use = n\n",
        "\n",
        "      # Trends for the various probability levels.\n",
        "      p01 = icdf_percent_n(dist, temp2cdf_hist, params, var=var, percent=0.01, n=n_to_use)\n",
        "      p05 = icdf_percent_n(dist, temp2cdf_hist, params, var=var, percent=0.05, n=n_to_use)\n",
        "      p50 = icdf_percent_n(dist, temp2cdf_hist, params, var=var, percent=0.5, n=n_to_use)\n",
        "      p95 = icdf_percent_n(dist, temp2cdf_hist, params, var=var, percent=0.95, n=n_to_use)\n",
        "\n",
        "      p95s.append(p95)\n",
        "      p50s.append(p50)\n",
        "      p05s.append(p05)\n",
        "      p01s.append(p01)\n",
        "\n",
        "\n",
        "    # Plot trend for the dist.\n",
        "    axs[i].plot(year_starts, p95s, 'yellowgreen', label='p95')\n",
        "    axs[i].plot(year_starts, p50s, 'mediumorchid', label='p50')\n",
        "    axs[i].plot(year_starts, p05s, 'darkorange', label='p05')\n",
        "    axs[i].plot(year_starts, p01s, 'maroon', label='p01')\n",
        "    if no_fitting:\n",
        "      axs[i].set_title(f\"{var}, No Distribution, N={n_to_use}\")\n",
        "    else:\n",
        "      axs[i].set_title(f\"{var}, {dist} fit, N={n_to_use}\")\n",
        "    axs[i].set_xlabel(\"Decade\")\n",
        "    axs[i].set_ylabel(f\"{var} (C)\")\n",
        "    axs[i].legend(loc=\"upper left\")\n",
        "    axs[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
        "\n",
        "    # Please the fitted PDF for the Gaussian Mixture.\n",
        "    if dist == \"GaussianMixture\":\n",
        "      for y_ind in range(len(year_starts)):\n",
        "        smoothed_pdf_y = generate_smooth_pdf(x_min, x_max, dist, fit_params[y_ind])\n",
        "        if len(smoothed_pdf_y):\n",
        "          axs[num_rows - 1].plot(x_array, smoothed_pdf_y, linewidth=1.5, color=colors_y[y_ind], label=f\"{year_starts[y_ind]}\")\n",
        "      axs[num_rows - 1].set_title(f\"{var}, GaussianMixture Fits Across Decades, N={n_to_use}\")\n",
        "      axs[num_rows - 1].set_xlabel(f\"{var} (C)\")\n",
        "      axs[num_rows - 1].set_ylabel(f\"Probability\")\n",
        "      axs[num_rows - 1].legend(loc=\"upper left\")\n",
        "\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "LqWlT77aFx7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-Computation (Takes a Minute or Two)\n",
        "\n",
        "The data for 100 counties across the US is made available as an extract on a Google Drive location. This data is retrieved directly and the distribution parameters are then estimated (here in this colab). The histogram fitting is precomputed (using SQL), the code for which is provided at: https://gist.github.com/pradh/2fda9a4e30f7fd9d0086ec685e94aeb5 (but not used directly in this Colab)."
      ],
      "metadata": {
        "id": "1VEx6Zm84-0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Counties List (Do Not Edit)\n",
        "COUNTIES_MAP = {\n",
        "  'Sarpy County': 'geoId/31153',\n",
        "  'Franklin County': 'geoId/39049',\n",
        "  'Ellis County': 'geoId/48139',\n",
        "  'Mayes County': 'geoId/40097',\n",
        "  'Kern County': 'geoId/06029',\n",
        "  'Atkinson County': 'geoId/13003',\n",
        "  'Pierce County': 'geoId/13229',\n",
        "  'Ware County': 'geoId/13299',\n",
        "  'Brantley County': 'geoId/13025',\n",
        "  'Telfair County': 'geoId/13271',\n",
        "  'Charlton County': 'geoId/13049',\n",
        "  'Imperial County': 'geoId/06025',\n",
        "  'Inyo County': 'geoId/06027',\n",
        "  'Riverside County': 'geoId/06065',\n",
        "  'Cheyenne County': 'geoId/08017',\n",
        "  'Highlands County': 'geoId/12055',\n",
        "  'Clinch County': 'geoId/13065',\n",
        "  'Wheeler County': 'geoId/13309',\n",
        "  'Garfield County': 'geoId/30033',\n",
        "  'Chesterfield County': 'geoId/45025',\n",
        "  'Okeechobee County': 'geoId/12093',\n",
        "  'Polk County': 'geoId/12105',\n",
        "  'Berrien County': 'geoId/13019',\n",
        "  'La Paz County': 'geoId/04012',\n",
        "  'Maricopa County': 'geoId/04013',\n",
        "  'Yuma County': 'geoId/04027',\n",
        "  'San Bernardino County': 'geoId/06071',\n",
        "  'Hardee County': 'geoId/12049',\n",
        "  'Osceola County': 'geoId/12097',\n",
        "  'Bacon County': 'geoId/13005',\n",
        "  'Coffee County': 'geoId/13069',\n",
        "  'Irwin County': 'geoId/13155',\n",
        "  'Jeff Davis County': 'geoId/13161',\n",
        "  'Lanier County': 'geoId/13173',\n",
        "  'Todd County': 'geoId/21219',\n",
        "  'Frontier County': 'geoId/31063',\n",
        "  'Union County': 'geoId/37179',\n",
        "  'Hill County': 'geoId/48217',\n",
        "  'McMullen County': 'geoId/48311',\n",
        "  'Montague County': 'geoId/48337',\n",
        "  'Dodge County': 'geoId/13091',\n",
        "  'Marlboro County': 'geoId/45069',\n",
        "  'Christian County': 'geoId/21047',\n",
        "  'Mohave County': 'geoId/04015',\n",
        "  'Pima County': 'geoId/04019',\n",
        "  'Crowley County': 'geoId/08025',\n",
        "  'Prowers County': 'geoId/08099',\n",
        "  'Hamilton County': 'geoId/12047',\n",
        "  'Hillsborough County': 'geoId/12057',\n",
        "  'Baker County': 'geoId/13007',\n",
        "  'Bulloch County': 'geoId/13031',\n",
        "  'Camden County': 'geoId/13039',\n",
        "  'Echols County': 'geoId/13101',\n",
        "  'Emanuel County': 'geoId/13107',\n",
        "  'Jefferson County': 'geoId/13163',\n",
        "  'Lowndes County': 'geoId/13185',\n",
        "  'Mitchell County': 'geoId/13205',\n",
        "  'Montgomery County': 'geoId/13209',\n",
        "  'Treutlen County': 'geoId/13283',\n",
        "  'Washington County': 'geoId/13303',\n",
        "  'Wayne County': 'geoId/13305',\n",
        "  'Wilcox County': 'geoId/13315',\n",
        "  'Scott County': 'geoId/18143',\n",
        "  'Logan County': 'geoId/21141',\n",
        "  'Trimble County': 'geoId/21223',\n",
        "  'Petroleum County': 'geoId/30069',\n",
        "  'Luna County': 'geoId/35029',\n",
        "  'Anson County': 'geoId/37007',\n",
        "  'Randolph County': 'geoId/37151',\n",
        "  'Richmond County': 'geoId/37153',\n",
        "  'Robeson County': 'geoId/37155',\n",
        "  'Beckham County': 'geoId/40009',\n",
        "  'Cherokee County': 'geoId/45021',\n",
        "  'Fairfield County': 'geoId/45039',\n",
        "  'Kershaw County': 'geoId/45055',\n",
        "  'Lancaster County': 'geoId/45057',\n",
        "  'Newberry County': 'geoId/45071',\n",
        "  'Saluda County': 'geoId/45081',\n",
        "  'Perkins County': 'geoId/46105',\n",
        "  'Brazos County': 'geoId/48041',\n",
        "  'Burleson County': 'geoId/48051',\n",
        "  'La Salle County': 'geoId/48283',\n",
        "  'Orange County': 'geoId/12095',\n",
        "  'Cook County': 'geoId/13075',\n",
        "  'Jenkins County': 'geoId/13165',\n",
        "  'Johnson County': 'geoId/13167',\n",
        "  'Laurens County': 'geoId/13175',\n",
        "  'Worth County': 'geoId/13321',\n",
        "  'Beaver County': 'geoId/40007',\n",
        "  'Butler County': 'geoId/01013',\n",
        "  'Choctaw County': 'geoId/01023',\n",
        "  'Coffee County': 'geoId/01031',\n",
        "  'Colbert County': 'geoId/01033',\n",
        "  'Covington County': 'geoId/01039',\n",
        "  'Dale County': 'geoId/01045',\n",
        "  'Dallas County': 'geoId/01047',\n",
        "  'Henry County': 'geoId/01067',\n",
        "  'Lauderdale County': 'geoId/01077',\n",
        "  'Marengo County': 'geoId/01091',\n",
        "  'Wilcox County': 'geoId/01131',\n",
        "  'Cochise County': 'geoId/04003',\n",
        "  'Otero County': 'geoId/08089',\n",
        "  'Columbia County': 'geoId/12023',\n",
        "  'Manatee County': 'geoId/12081',\n",
        "}"
      ],
      "metadata": {
        "id": "nj4MhvJBSrks",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Retrieve Data Extracts (takes some time)\n",
        "\n",
        "# Data files available at: # https://drive.google.com/drive/folders/1IrNkjewMq_0aLyeaKpvnsA5-Y01_k32B\n",
        "df_p1d_max_temp_future = get_data_extract('1251dC0opkFURYqQnpSL2f36BmCeUqxiH')\n",
        "df_p1d_max_temp_past = get_data_extract('1jTaS8OFMKWRaJIWRNHp7rKKdMoT0nlUP')\n",
        "\n",
        "df_p1d_min_temp_future = get_data_extract('1_GtY6kzn_eVvSJjD2TjZN-9jSIT1jWC4')\n",
        "df_p1d_min_temp_past = get_data_extract('1s4GSXtWH8o0kDizzZ8xUZCs-a4v83-9B')\n",
        "\n",
        "df_hist_max_temp = get_data_extract('1A-WekLGrzuyvbApEIev88Kyi-uQ23_af')\n",
        "df_hist_min_temp = get_data_extract('1cLuvoWUwCwRz5bb0Uhv9cxr4nbSIG0xO')\n",
        "\n",
        "df_hist_max_temp.rename(columns={\"decade\": \"year\", \"model\": \"cmip6_model\"}, inplace=True)\n",
        "df_hist_min_temp.rename(columns={\"decade\": \"year\", \"model\": \"cmip6_model\"}, inplace=True)\n",
        "\n",
        "data_p1d = {\n",
        "    (\"Max_Temperature\", \"future\"): df_p1d_max_temp_future,\n",
        "    (\"Max_Temperature\", \"past\"): df_p1d_max_temp_past,\n",
        "    (\"Min_Temperature\", \"future\"): df_p1d_min_temp_future,\n",
        "    (\"Min_Temperature\", \"past\"): df_p1d_min_temp_past,\n",
        "}\n",
        "data_hist = {\n",
        "    \"Max_Temperature\": df_hist_max_temp,\n",
        "    \"Min_Temperature\": df_hist_min_temp,\n",
        "}\n",
        "\n",
        "# Actual Observations (from 1980-2020).\n",
        "df_actual = get_data_extract('1q8HmVM1r02FE5DSrDlrx0T20_SzHhepF')\n",
        "df_actual[\"year\"] = df_actual.date.str.slice(0, 4)\n",
        "df_actual[\"year\"] = df_actual[\"year\"].astype('int')"
      ],
      "metadata": {
        "id": "rmFURQHk-yhb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitted Distributions; Probability Thresholds"
      ],
      "metadata": {
        "id": "tyep7ThVj0jR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Specify a county, variable, scenario, cmip6 model, start year (decade) and specify a positive integer N (read the preamble for an explanation).\n",
        "#@markdown <br><b>Note: That the SSP245, SSP585 scenarios are only applicable post-2020 while the 'historical' scenario is only applicable prior to 2010.</b>\n",
        "\n",
        "County = \"Imperial County\" #@param ['Anson County', 'Atkinson County', 'Bacon County', 'Baker County', 'Beaver County', 'Beckham County', 'Berrien County', 'Brantley County', 'Brazos County', 'Bulloch County', 'Burleson County', 'Butler County', 'Camden County', 'Charlton County', 'Cherokee County', 'Chesterfield County', 'Cheyenne County', 'Choctaw County', 'Christian County', 'Clinch County', 'Cochise County', 'Coffee County', 'Colbert County', 'Columbia County', 'Cook County', 'Covington County', 'Crowley County', 'Dale County', 'Dallas County', 'Dodge County', 'Echols County', 'Ellis County', 'Emanuel County', 'Fairfield County', 'Franklin County', 'Frontier County', 'Garfield County', 'Hamilton County', 'Hardee County', 'Henry County', 'Highlands County', 'Hill County', 'Hillsborough County', 'Imperial County', 'Inyo County', 'Irwin County', 'Jeff Davis County', 'Jefferson County', 'Jenkins County', 'Johnson County', 'Kern County', 'Kershaw County', 'La Paz County', 'La Salle County', 'Lancaster County', 'Lanier County', 'Lauderdale County', 'Laurens County', 'Logan County', 'Lowndes County', 'Luna County', 'Manatee County', 'Marengo County', 'Maricopa County', 'Marlboro County', 'Mayes County', 'McMullen County', 'Mitchell County', 'Mohave County', 'Montague County', 'Montgomery County', 'Newberry County', 'Okeechobee County', 'Orange County', 'Osceola County', 'Otero County', 'Perkins County', 'Petroleum County', 'Pierce County', 'Pima County', 'Polk County', 'Prowers County', 'Randolph County', 'Richmond County', 'Riverside County', 'Robeson County', 'Saluda County', 'San Bernardino County', 'Sarpy County', 'Scott County', 'Telfair County', 'Todd County', 'Treutlen County', 'Trimble County', 'Union County', 'Ware County', 'Washington County', 'Wayne County', 'Wheeler County', 'Wilcox County', 'Worth County', 'Yuma County']\n",
        "Variable = \"Max_Temperature\" #@param [\"Max_Temperature\", \"Min_Temperature\"]\n",
        "Scenario = 'historical' #@param [\"SSP245\", \"SSP585\", \"historical\"]\n",
        "Model = 'Ensemble' #@param ['Ensemble', 'GFDL-CM4','GFDL-ESM4','HADGEM3-GC31-LL','HADGEM3-GC31-MM','MPI-ESM1-2-HR','MPI-ESM1-2-LR']\n",
        "Decade_Start = '2000' #@param [\"2040\", \"2030\", \"2020\", \"2000\", \"1990\", \"1980\"]\n",
        "N = \"52\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "year_start = int(Decade_Start)\n",
        "year_end = year_start + 10\n",
        "proceed = False\n",
        "try:\n",
        "  N = int(N)\n",
        "  if N > 0:\n",
        "    proceed = True\n",
        "except:\n",
        "  pass\n",
        "\n",
        "if not N:\n",
        "  print(\"N must be a positive integer\")\n",
        "\n",
        "county_dcid = COUNTIES_MAP[County]\n",
        "\n",
        "print()\n",
        "print(f\"Place Page ({County}): https://datacommons.org/place/{county_dcid}\")\n",
        "print()\n",
        "\n",
        "hist_scenario = Scenario\n",
        "tuple_1 = Variable\n",
        "tuple_2 = \"future\"\n",
        "if Scenario == \"historical\":\n",
        "  hist_scenario = ''\n",
        "  tuple_2 = \"past\"\n",
        "\n",
        "if tuple_2 == \"future\":\n",
        "  print(\"Note: there is no actual observed data for this decade\")\n",
        "\n",
        "df_p1d = data_p1d[(tuple_1, tuple_2)]\n",
        "df_hist = data_hist[tuple_1]\n",
        "\n",
        "# Get the Data from BQ.\n",
        "if proceed:\n",
        "  df_all_extract = df_p1d[(df_p1d[\"year\"] >= year_start) & (df_p1d[\"year\"] < year_end) &\n",
        "   (df_p1d['scenario'] == Scenario) & (df_p1d['county'] == county_dcid)]\n",
        "\n",
        "  if Model != 'Ensemble':\n",
        "    df_all_extract = df_all_extract[df_all_extract[\"cmip6_model\"] == Model]\n",
        "\n",
        "  df_hist_extract = df_hist[(df_hist[\"year\"] >= year_start) & (df_hist[\"year\"] < year_end) &\n",
        "   (df_hist['scenario'] == hist_scenario) & (df_hist['county'] == county_dcid) & (df_hist['cmip6_model'] == Model)]\n",
        "\n",
        "  df_actual_extract = df_actual[(df_actual[\"year\"] >= year_start) &\n",
        "                                (df_actual[\"year\"] < year_end) &\n",
        "                                (df_actual[\"dcid\"] == county_dcid)]\n",
        "  if (not df_all_extract.empty) and (not df_hist_extract.empty):\n",
        "    plot_fits(df_all_extract, df_hist_extract, df_actual_extract, County, N, year_start, year_end, Scenario, Model, Variable)\n",
        "  else:\n",
        "    print(\"***** This combination of Variable, Scenario and Model is not valid. Please try a different combination. ******\")\n",
        "    print(len(df_all_extract), len(df_hist_extract))\n"
      ],
      "metadata": {
        "id": "FlR1Ejk4eEbj",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trends Over Decades: Proability Levels; Mixture Distribution Fits"
      ],
      "metadata": {
        "id": "1yiwQbxTFk_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Specify a county, variable, scenario, cmip6 model, and specify a positive integer N (read the preamble for an explanation).\n",
        "#@markdown <br><b>Note: Before 2010, only historical runs are used and after 2010 the selection SSP scenario is used.</b>\n",
        "\n",
        "County = \"Imperial County\" #@param ['Anson County', 'Atkinson County', 'Bacon County', 'Baker County', 'Beaver County', 'Beckham County', 'Berrien County', 'Brantley County', 'Brazos County', 'Bulloch County', 'Burleson County', 'Butler County', 'Camden County', 'Charlton County', 'Cherokee County', 'Chesterfield County', 'Cheyenne County', 'Choctaw County', 'Christian County', 'Clinch County', 'Cochise County', 'Coffee County', 'Colbert County', 'Columbia County', 'Cook County', 'Covington County', 'Crowley County', 'Dale County', 'Dallas County', 'Dodge County', 'Echols County', 'Ellis County', 'Emanuel County', 'Fairfield County', 'Franklin County', 'Frontier County', 'Garfield County', 'Hamilton County', 'Hardee County', 'Henry County', 'Highlands County', 'Hill County', 'Hillsborough County', 'Imperial County', 'Inyo County', 'Irwin County', 'Jeff Davis County', 'Jefferson County', 'Jenkins County', 'Johnson County', 'Kern County', 'Kershaw County', 'La Paz County', 'La Salle County', 'Lancaster County', 'Lanier County', 'Lauderdale County', 'Laurens County', 'Logan County', 'Lowndes County', 'Luna County', 'Manatee County', 'Marengo County', 'Maricopa County', 'Marlboro County', 'Mayes County', 'McMullen County', 'Mitchell County', 'Mohave County', 'Montague County', 'Montgomery County', 'Newberry County', 'Okeechobee County', 'Orange County', 'Osceola County', 'Otero County', 'Perkins County', 'Petroleum County', 'Pierce County', 'Pima County', 'Polk County', 'Prowers County', 'Randolph County', 'Richmond County', 'Riverside County', 'Robeson County', 'Saluda County', 'San Bernardino County', 'Sarpy County', 'Scott County', 'Telfair County', 'Todd County', 'Treutlen County', 'Trimble County', 'Union County', 'Ware County', 'Washington County', 'Wayne County', 'Wheeler County', 'Wilcox County', 'Worth County', 'Yuma County']\n",
        "Variable = \"Max_Temperature\" #@param [\"Max_Temperature\", \"Min_Temperature\"]\n",
        "Scenario = 'SSP585' #@param [\"SSP245\", \"SSP585\"]\n",
        "Model = 'Ensemble' #@param ['Ensemble', 'GFDL-CM4','GFDL-ESM4','HADGEM3-GC31-LL','HADGEM3-GC31-MM','MPI-ESM1-2-HR','MPI-ESM1-2-LR']\n",
        "N = \"520\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "proceed = False\n",
        "try:\n",
        "  N = int(N)\n",
        "  if N > 0:\n",
        "    proceed = True\n",
        "except:\n",
        "  pass\n",
        "\n",
        "if not N:\n",
        "  print(\"N must be a positive integer\")\n",
        "\n",
        "county_dcid = COUNTIES_MAP[County]\n",
        "\n",
        "print()\n",
        "print(f\"Place Page ({County}): https://datacommons.org/place/{county_dcid}\")\n",
        "print()\n",
        "\n",
        "if proceed:\n",
        "  plot_trends(data_p1d, data_hist, county_dcid, Scenario, Model, Variable, N)\n"
      ],
      "metadata": {
        "id": "3Se--BR6Fjrz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}