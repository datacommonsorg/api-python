{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification and Model Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datacommonsorg/api-python/blob/master/notebooks/intro_data_science/Classification_and_Model_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2022 Google LLC.\n",
        "SPDX-License-Identifier: Apache-2.0"
      ],
      "metadata": {
        "id": "vTbNks0XH5xW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go4bM4R_LeId"
      },
      "source": [
        "# Classification and Model Evaluation\n",
        "\n",
        "So you've built a machine learning model, or perhaps multiple models... now what? How do you know if those models are any good? And if you have multiple candidate models, how should you choose which one to utimately deploy?\n",
        "\n",
        "The answer: model evaluation. Understanding how to evaluate your models is an essential skill not just to check how well your models perform, but also to diagnose issues and find areas for improvement. Most importantly, we need to understand whether or not we can trust our model's predictions.\n",
        "\n",
        "## Learning Objectives\n",
        "In this lesson, we'll be covering:\n",
        "\n",
        "* Model Selection\n",
        "* Generalization and Overfitting\n",
        "* Train/Test Splits\n",
        "* Cross Validation\n",
        "* Statistical Evaulation Metrics:\n",
        "* How do we know a model is \"good\"?\n",
        "* Tradeoffs between evaluation metrics\n",
        "\n",
        "---\n",
        "**Need extra help?**\n",
        "\n",
        "If you're new to Google Colab, take a look at [this getting started tutorial](https://colab.research.google.com/notebooks/intro.ipynb).\n",
        "\n",
        "To build more familiarity with the Data Commons API, check out these [Data Commons Tutorials](https://docs.datacommons.org/tutorials/).\n",
        "\n",
        "And for help with Pandas and manipulating data frames, take a look at the [Pandas Documentation](https://pandas.pydata.org/docs/reference/index.html).\n",
        "\n",
        "We'll be using the scikit-learn library for implementing our models today. Documentation can be found [here](https://scikit-learn.org/stable/modules/classes.html). \n",
        "\n",
        "As usual, if you have any other questions, please reach out to your course staff!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umu23IaLzlMJ"
      },
      "source": [
        "## Part 0: Introduction and Setup\n",
        "\n",
        "The [obesity epidemic in the United States](https://en.wikipedia.org/wiki/Obesity_in_the_United_States) is a major public health issue. Obesity rates vary across the nation by geographic location. In this colab, we'll be exploring how obesity rates vary with different health or societal factors across US cities.\n",
        "\n",
        "**Our Data Science Question:** Can we predict which cities have high (>30%) or low (<30%) obesity rates based on other health or lifestyle factors?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpW0UwpG0ytY"
      },
      "source": [
        "\"\"\"\n",
        "Install Data Commons API\n",
        "\n",
        "We need to install the Data Commons API, since they don't ship natively with\n",
        "most python installations.\n",
        "\n",
        "In Colab, we'll be installing the Data Commons python and pandas APIs \n",
        "through pip.\n",
        "\"\"\"\n",
        "\n",
        "!pip install datacommons --upgrade --quiet\n",
        "!pip install datacommons_pandas --upgrade --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdh_OcdOIYGD"
      },
      "source": [
        "\"\"\"\n",
        "Imports\n",
        "\n",
        "This is where we'll load all the libraries we need for this assignment.\n",
        "\"\"\"\n",
        "\n",
        "# Data Commons Python and Pandas APIs\n",
        "import datacommons\n",
        "import datacommons_pandas\n",
        "\n",
        "# For manipulating data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For implementing models and evaluation methods\n",
        "from sklearn import linear_model, svm, tree\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# For plotting\n",
        "from matplotlib import pyplot as plt\n",
        "from mlxtend.plotting import plot_decision_regions, category_scatter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahmaNIig4Vzv"
      },
      "source": [
        "\"\"\"\n",
        "Loading the Data\n",
        "\n",
        "We'll query data using the Data Commons API, storing it in a Pandas data frame\n",
        "\"\"\"\n",
        "\n",
        "city_dcids = datacommons.get_property_values([\"CDC500_City\"],\n",
        "                                             \"member\",\n",
        "                                             limit=500)[\"CDC500_City\"]\n",
        "\n",
        "# We've compiled a list of some nice Data Commons Statistical Variables\n",
        "# to use as features for you\n",
        "stat_vars_to_query = [\n",
        "                      \"Count_Person\",\n",
        "                      \"Median_Income_Person\",\n",
        "                      \"Count_Person_NoHealthInsurance\",\n",
        "                      \"Percent_Person_PhysicalInactivity\",\n",
        "                      \"Percent_Person_SleepLessThan7Hours\",\n",
        "                      \"dc/e9gftzl2hm8h9\", # Commute Time, this has a weird DCID\n",
        "                      \"Percent_Person_WithHighBloodPressure\",\n",
        "                      \"Percent_Person_WithMentalHealthNotGood\",\n",
        "                      \"Percent_Person_WithHighCholesterol\",\n",
        "                      \"Percent_Person_Obesity\"\n",
        "                      \n",
        "]\n",
        "\n",
        "# Query Data Commons for the data and display the data\n",
        "raw_features_df = datacommons_pandas.build_multivariate_dataframe(city_dcids,stat_vars_to_query)\n",
        "display(raw_features_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKzqKCCcEuFO"
      },
      "source": [
        "We've succesfully loaded our data, but there are still a couple preprocessing steps to go through first. Specifically, we're going to:\n",
        "\n",
        "1. Change the row labels from dcids to names for readability.\n",
        "\n",
        "2. Change the column name \"`dc/e9gftzl2hm8h9`\" to the more human readable \"`Commute_Time`\"\n",
        "\n",
        "3. The raw commute time values from Data Commons shows the total amount of minutes spent for everyone in the city. Let's instead look at the average commute time for a single person, which we'll get by dividing the raw commute time (`Commute_Time`) by population size (`Count_Person`)\n",
        "\n",
        "4. Similarly, we'll get a `Percent_NoHealthInsurance` by dividing the the count of people without health insurance (`Count_Person_NoHealthInsurance`) by population size.\n",
        "\n",
        "5. To perform classification, we need to convert our obesity rate data into labels. In this lesson, we'll look at binary classification, and will split our cities into  \"Low obesity rate\" (label 0, obesity% < 30%) and \"High obesity rate\" (label 1, obesity% >= 30%) categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxXQ3Kw3FB6H"
      },
      "source": [
        "# Make Row Names More Readable\n",
        "# --- First, we'll copy the dcids into their own column\n",
        "# --- Next, we'll get the \"name\" property of each dcid\n",
        "# --- Then add the returned dictionary to our data frame as a new column\n",
        "# --- Finally, we'll set this column as the new index\n",
        "df = raw_features_df.copy(deep=True)\n",
        "df['DCID'] = df.index\n",
        "city_name_dict = datacommons.get_property_values(city_dcids, 'name')\n",
        "city_name_dict = {key:value[0] for key, value in city_name_dict.items()}\n",
        "df['City'] = pd.Series(city_name_dict)\n",
        "df.set_index('City', inplace=True)\n",
        "\n",
        "# Rename column \"dc/e9gftzl2jm8h9\" to \"Commute_Time\"\n",
        "df.rename(columns={\"dc/e9gftzl2hm8h9\":\"Commute_Time\"}, inplace=True)\n",
        "\n",
        "# Convert commute_time value\n",
        "avg_commute_time = df[\"Commute_Time\"]/df[\"Count_Person\"]\n",
        "df[\"Commute_Time\"] = avg_commute_time\n",
        "\n",
        "# Convert Count of No Health Insurance to Percentage\n",
        "percent_noHealthInsurance = df[\"Count_Person_NoHealthInsurance\"]/df[\"Count_Person\"]\n",
        "df[\"Percent_NoHealthInsurance\"] = percent_noHealthInsurance\n",
        "\n",
        "# Create labels based on the Obesity rate of each city\n",
        "# --- Percent_Person_Obesity < 30 will be Label 0\n",
        "# --- Percent_Person_Obesity >= 30 will be label 1\n",
        "df[\"Label\"] = df['Percent_Person_Obesity'] >= 30.0\n",
        "df[\"Label\"] = df[\"Label\"].astype(int)\n",
        "\n",
        "# Display results\n",
        "display(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un0pOqmC2l01"
      },
      "source": [
        "Now that we have our features and labels set, it's time to start modeling!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K38HUB5uz2Mg"
      },
      "source": [
        "## 1) Model Selection\n",
        "\n",
        "The results of our models are only good if our models are correct in the first place. \"Good\" here can mean different things depending on your application -- we'll talk more about that later in this assignment.\n",
        "\n",
        "What's important to know for now is that the conclusions we draw are subject to the assumptions and limitations of our underlying model. Thus, making sure we choose the right models to analyze is important! But how does one choose the right model?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IDNDln0961H"
      },
      "source": [
        "### 1.1) Building Intuition -- Which model do you think is best?\n",
        "To build some intuition, let's start off with a simple example. We'll use a subset of our data. For ease of visualization, we'll start with just two features, and just 10 cities.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_53hMUL_HS3"
      },
      "source": [
        "# For ease of visualization, we'll focus on just a few cities\n",
        "subset_city_dcids = [\"geoId/0667000\", # San Francisco, CA\n",
        "                     \"geoId/3651000\", # NYC, NY\n",
        "                     \"geoId/1304000\", # Atlanta, GA\n",
        "                     \"geoId/2404000\", # Baltimore, MD\n",
        "                     \"geoId/3050200\", # Missoula, MT\n",
        "                     \"geoId/4835000\", # Houston, TX\n",
        "                     \"geoId/2622000\", # Detroit, MI\n",
        "                     \"geoId/5363000\", # Seattle, WA\n",
        "                     \"geoId/2938000\", # Kansas City, MO\n",
        "                     \"geoId/4752006\"  # Nashville, TN\n",
        "                    ]\n",
        "\n",
        "# Create a subset data frame with just those cities\n",
        "subset_df = df.loc[df['DCID'].isin(subset_city_dcids)]\n",
        "\n",
        "# We'll just use 2 features for ease of visualization\n",
        "X = subset_df[[\"Percent_Person_PhysicalInactivity\",\n",
        "                 \"Percent_Person_SleepLessThan7Hours\"]]\n",
        "Y = subset_df[['Label']]\n",
        "\n",
        "# Visualize the data\n",
        "colors = ['#1f77b4', '#ff7f0e']\n",
        "markers = ['s', '^']\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Original Data')\n",
        "ax.set_ylabel('Percent_Person_SleepLessThan7Hours')\n",
        "ax.set_xlabel('Percent_Person_PhysicalInactivity')\n",
        "for i in range(X.shape[0]):\n",
        "  ax.scatter(X[\"Percent_Person_PhysicalInactivity\"][i],\n",
        "             X[\"Percent_Person_SleepLessThan7Hours\"][i],\n",
        "             c=colors[Y['Label'][i]],\n",
        "             marker=markers[Y['Label'][i]],\n",
        "            )\n",
        "ax.legend([0, 1])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-_ZAW1XJV5h"
      },
      "source": [
        "\n",
        "The following blocks of code will generate 2 candidate classifiers, for labeling the datapoints as either label 0 (high obesity rate), or label 1 (high obesity rate).\n",
        "\n",
        "The code will also output an accuracy score, which for this section is defined as: \n",
        "\n",
        "$\\text{Accuracy} = \\frac{\\text{# correctly labeled}}{\\text{# total datapoints}}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4IrknfmLJfb"
      },
      "source": [
        "# Classifier 1\n",
        "classifier1 = svm.SVC()\n",
        "classifier1.fit(X, Y[\"Label\"])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Classifier 1')\n",
        "ax.set_ylabel('Percent_Person_SleepLessThan7Hours')\n",
        "ax.set_xlabel('Percent_Person_PhysicalInactivity')\n",
        "plot_decision_regions(X.to_numpy(),\n",
        "                      Y[\"Label\"].to_numpy(),\n",
        "                      clf=classifier1,\n",
        "                      legend=2)\n",
        "plt.show()\n",
        "\n",
        "print('Accuracy of this classifier is:', classifier1.score(X,Y[\"Label\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN7LCX0YLL3n"
      },
      "source": [
        "# Classifier 2\n",
        "classifier2 = tree.DecisionTreeClassifier(random_state=0)\n",
        "classifier2.fit(X, Y[\"Label\"])\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Classifier 2')\n",
        "ax.set_ylabel('Percent_Person_SleepLessThan7Hours')\n",
        "ax.set_xlabel('Percent_Person_PhysicalInactivity')\n",
        "plot_decision_regions(X.to_numpy(),\n",
        "                      Y[\"Label\"].to_numpy(),\n",
        "                      clf=classifier2,\n",
        "                      legend=2)\n",
        "plt.show()\n",
        "\n",
        "print('Accuracy of this classifier is:', classifier2.score(X,Y[\"Label\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFJOOjyOVje8"
      },
      "source": [
        "**1A)** Which model do you think is better, Classifier 1 or Classifier 2? Explain your reasoning.\n",
        "\n",
        "**1B)** Classifier 2 has a higher accuracy than Classifier 1, but has a more complicated decision boundary. Which do you think would generalize best to new data?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrWpvHTNU9YT"
      },
      "source": [
        "\n",
        "### 1.2) The Importance of Generalizability\n",
        "\n",
        "So, how did we do? Let's see what happens when we add back the rest of the cities (we'll keep using just the 2 features for ease of visualization.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ9ZFhPqgkmJ"
      },
      "source": [
        "# Original Data\n",
        "X_full = df[[\"Percent_Person_PhysicalInactivity\",\n",
        "                 \"Percent_Person_SleepLessThan7Hours\"]]\n",
        "Y_full = df[['Label']]\n",
        "\n",
        "# Visualize the data\n",
        "cCycle = ['#1f77b4', '#ff7f0e']\n",
        "mCycle = ['s', '^']\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Original Data')\n",
        "ax.set_ylabel('Percent_Person_SleepLessThan7Hours')\n",
        "ax.set_xlabel('Percent_Person_PhysicalInactivity')\n",
        "for i in range(X_full.shape[0]):\n",
        "  ax.scatter(X_full[\"Percent_Person_PhysicalInactivity\"][i],\n",
        "             X_full[\"Percent_Person_SleepLessThan7Hours\"][i],\n",
        "             c=cCycle[Y_full['Label'][i]],\n",
        "             marker=mCycle[Y_full['Label'][i]],\n",
        "            )\n",
        "ax.legend([0, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEz4hyJfg_F8"
      },
      "source": [
        "# Classifier 1\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Classifier 1')\n",
        "ax.set_ylabel('Percent_Person_SleepLessThan7Hours')\n",
        "ax.set_xlabel('Percent_Person_PhysicalInactivity')\n",
        "plot_decision_regions(X_full.to_numpy(),\n",
        "                      Y_full[\"Label\"].to_numpy(),\n",
        "                      clf=classifier1,\n",
        "                      legend=2)\n",
        "plt.show()\n",
        "\n",
        "print('Accuracy of this classifier is: %.2f' % classifier1.score(X_full,Y_full[\"Label\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkAC1ii9hZ62"
      },
      "source": [
        "# Classifier 2\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Classifier 2')\n",
        "ax.set_ylabel('Percent_Person_SleepLessThan7Hours')\n",
        "ax.set_xlabel('Percent_Person_PhysicalInactivity')\n",
        "plot_decision_regions(X_full.to_numpy(),\n",
        "                      Y_full[\"Label\"].to_numpy(),\n",
        "                      clf=classifier2,\n",
        "                      legend=2)\n",
        "plt.show()\n",
        "\n",
        "print('Accuracy of this classifier is: %.2f' % classifier2.score(X_full,Y_full[\"Label\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51H3zw11xMnF"
      },
      "source": [
        "**2A)** In light of all the new datapoints, now which classifier do you think is better, Classifer 1 or Classifier 2? Explain your reasoning.\n",
        "\n",
        "**2B)** In Question 1, Classifier 1 had a *lower* accuracy than Classifier 2. After adding more datapoints, we now see the reverse, with Classifier 1 having a *higher* accuracy than Classifier 2. What happened? Give an explanation (or at least your best guess) for why this is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnGbwAsQs-A9"
      },
      "source": [
        "## 2) Evaluation Metrics\n",
        "\n",
        "In question 1, we were able to visualize how well our models performed by plotting our data and decision boundaries. However, this was only possible because we limited ourselves to just 2 features. Unfortunately for us, humans are only good at visualizing up to 3 dimesnions. As you increase the number of features and/or complexity of your models, creating meaningful visualizations quickly become intractable.\n",
        "\n",
        "Thus, we'll need other methods to measure how well our models perform. In this section, we'll cover some common strategies for evaluating models.\n",
        "\n",
        "To start, let's finally fit a model to all of our available data (e.g. 500 cities and 8 features). Because the features have different scales, we'll also take care to standardize their values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OEKYfmWClw5"
      },
      "source": [
        "# Use all features that aren't obesity\n",
        "X_large = df.dropna()[[\n",
        "              \"Median_Income_Person\",\n",
        "              \"Percent_NoHealthInsurance\",\n",
        "              \"Percent_Person_PhysicalInactivity\",\n",
        "              \"Percent_Person_SleepLessThan7Hours\",\n",
        "              \"Percent_Person_WithHighBloodPressure\",\n",
        "              \"Percent_Person_WithMentalHealthNotGood\",\n",
        "              \"Percent_Person_WithHighCholesterol\",\n",
        "              \"Commute_Time\"\n",
        "              ]]\n",
        "Y_large = df.dropna()[\"Label\"]\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler().fit(X_large)\n",
        "X_large = scaler.transform(X_large)\n",
        "\n",
        "# Create a model\n",
        "large_model = linear_model.Perceptron()\n",
        "large_model.fit(X_large, Y_large)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsxEsROd2o7z"
      },
      "source": [
        "### 2.1) Accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaRWM46b9zmX"
      },
      "source": [
        "#### 2.1.1) Classification Accuracy\n",
        "We've seen an example of an evaluation metric already -- accuracy! The accuracy score used question 1 is more commonly known as _classification accuracy_, and is the most common metric used in classification problems.\n",
        "\n",
        "As a refresher, the classification accuracy is the ratio of number of correct predictions to the total number of datapoints.\n",
        "\n",
        "**classification accuracy:**\n",
        "$Accuracy = \\frac{\\text{# correctly labeled}}{\\text{# total datapoints}}$\n",
        "\n",
        "Note that sometimes the classification accuracy can be misleading! Consider the following scenario:\n",
        "\n",
        "> There are two classes, A and B. We have 100 data points in our dataset. Of these 100 data points, 99 points are labeled class A, while only 1 of the data points are labeled class B.\n",
        "\n",
        "**2.1A)** Consider a model that always predicts class A. What is the accuracy of this always-A model?\n",
        "\n",
        "**2.1B)** How well do you expect the always-A model to perform on new, previously unseen data? Assume the new data follows the same distribution as the original 100 data points.\n",
        "\n",
        "**2.1C)** Run the following code block to calculate the classification accuracy of our large model. Is the accuracy higher or lower than you expected?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLdGp_nUFhPZ"
      },
      "source": [
        "print('Accuracy of the large model is: %.2f' % large_model.score(X_large,Y_large))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDiuNmdDYZ6L"
      },
      "source": [
        "### 2.2) Train/Test Splits\n",
        "The ability of a model to perform well on new, previously unseen data (drawn from the same distribution as the data used the create the model) is called _**Generalization**_. For most applications, we prefer models that generalize well over those that don't.\n",
        "\n",
        "One way to check the generalizability of a model is to perform an analysis similar to what we did in question 1. We'll take our data, and randomly split it into two subsets, a _**training set**_ that we'll use to build our model, and a _**test set**_, which we'll hold out until the model is complete and use it to evaluate how well our model can generalize to simulate new, previously unseen data.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6edxvroQl_G"
      },
      "source": [
        "#### 2.2.1) Choosing Split Sizes\n",
        "But what percentage of our datapoints should go into our training and test sets respectively? There are no hard and fast rules for this, the right split often depends on the application and how much data we have. The next few questions explore the key tradeoffs:\n",
        "\n",
        "**2.2A)** Consider a scenario with 5 data points in the training set and 95 data points in the test set. How accurate of a model do you think we're likely to train? \n",
        "\n",
        "**2.2B)** Does your answer to 2.2A change if we have 500 training and 9500 test points instead?\n",
        "\n",
        "**2.2C)** Consider a scenario with 95 data points in the training set and 5 data points in the test set. Is the test accuracy still a good measure of generalizability?\n",
        "\n",
        "**2.2D)** Does your answer to 2.2C change if we have 9500 training and 500 test points instead?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IKTxtfK7VE0"
      },
      "source": [
        "#### 2.2.2) Try for yourself!\n",
        "**2.2E)** Play around with a couple values of `test_size` in the code box below. Find a split ratio that seems to work well, and report what that ratio is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Pf04BU9e_i"
      },
      "source": [
        "'''\n",
        "Try a variety of different splits by changing the test_size variable, which\n",
        "represents the ratio of points to use in the test set.\n",
        "\n",
        "For example, for a 75% Training, 25% Test split, use test_size=0.25\n",
        "'''\n",
        "\n",
        "\n",
        "test_size = 0.25   # Change me! Enter a value between 0 and 1\n",
        "\n",
        "\n",
        "\n",
        "print(f'{np.round((1-test_size)*100)}% Training, {(test_size)*100}% Test Split' )\n",
        "\n",
        "# Randomly split data into Train and Test Sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_large, Y_large, test_size=test_size)\n",
        "\n",
        "# Fit a model on the training set\n",
        "large_model.fit(x_train, y_train)\n",
        "print('The TRAINING accuracy is: %.2f' % large_model.score(x_train, y_train))\n",
        "\n",
        "# Evaluate on the test Set\n",
        "print('The TEST accuracy is: %.2f' % large_model.score(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-wTIQ0OAWzd"
      },
      "source": [
        "#### 2.2.3) Training vs. Test Accuracy\n",
        "\n",
        "As you may have noticed from 2.2.2, we can calculate two different accuracies after performing a train/test split. A _**training accuracy**_ based on how well the model performs on the data it was trained on, and a _**test accuracy**_ based on how well the model performs on held out data. Typically, we select models based on _test_ accuracy. Afterall, a model's performance on new data after being deployed is usually more important than how well that model performed on the training data.\n",
        "\n",
        "So why measure training accuracy at all? It turns out training accuracy is often useful for diagnosing some common issues with models.\n",
        "\n",
        "For example, consider the following scenario:\n",
        "\n",
        ">After performing a train/test split, a model is found to have 100% training accuracy, but only 33% test accuracy.\n",
        "\n",
        "**2.2F)** What's going on with the model in the scenario? Come up with a hypothetical setup that could result in these train and test accuracies.\n",
        "\n",
        "_Hint: This situation is called **overfitting**. If you're stuck, feel free to look it up!_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFlqkyen9iAX"
      },
      "source": [
        "### 2.3) Cross-Validation\n",
        "\n",
        "If you haven't already, run the code box in 2.2.2 multiple times without changing the `test_size` variable. Notice how the accuracies can be different between runs?\n",
        "\n",
        "The problem is that each time we randomly select a train/test split, sometimes we'll get luckier or unlucky with a particular distribution of training or test data. To borrow a term from statistics, a sample size of $n=1$ is too small! We can do better.\n",
        "\n",
        "To get a better estimate of test accuracy, a common strategy is to use _**k-fold cross-validation**_. The general proceedure is:\n",
        "\n",
        "1. Split the data into $k$ groups.\n",
        "2. Then for each group (called a fold):\n",
        "  1. hold that group out as the test set, and use the remaining groups as a training set.\n",
        "  2. Fit a new model on the training set and record the resulting accuracy on the test set.\n",
        "3. Take the average of all test accuracies.\n",
        "\n",
        "#### A Note on Choosing k\n",
        "The number of folds to use depends on your data. Setting the number of folds implicitly also sets your train/test split ratio. For example, using 10 folds implies 10 (90% train, 10% test) splits. Common choices are $k=10$ or $k=5$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN5BAsFc9hfn"
      },
      "source": [
        "'''\n",
        "Set the number of folds by changing k.\n",
        "'''\n",
        "k = 5 # Enter an integer >=2. Number of folds.\n",
        "\n",
        "print(f'Test accuracies for {k} splits:')\n",
        "scores = cross_val_score(large_model, X_large, Y_large, cv=k)\n",
        "for i in range(k):\n",
        "  print('\\tFold %d: %.2f' % (i+1, scores[i]))\n",
        "print('Average score across all folds: %.2f' % np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iOF5tOUJByn"
      },
      "source": [
        "**2.3A)** Play around with the code box above to find a good value of $k$. What happens if $k$ is very large or very small?\n",
        "\n",
        "**2.2B)** How does the average score across all folds change with $k$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg0bLVujHOQb"
      },
      "source": [
        "### 2.4) Other Metrics Worth Knowing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw8y60E1N7AV"
      },
      "source": [
        "#### 2.4.1) What about Regression? -- Mean Squared Error\n",
        "Different models and different problems often use different accuracy metrics. You may have noticed classification accuracy doesn't make much sense for regression problems, where instead of predicting a label, the model predicts a numeric value. In regression, a common accuracy metric is the Mean Squared Error, or MSE.\n",
        "\n",
        "$ MSE = \\frac{1}{\\text{# total data points}}\\sum_{\\text{all data points}}(\\text{predicted value} - \\text{actual value})^2$\n",
        "\n",
        "It is a measure of the average difference between the predicted value and the actual value. The square ($^2$) can seem counterintuitive at first, but offers some nice mathematical properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxSp1xeIMTg8"
      },
      "source": [
        "#### 2.4.2) More Classification Metrics\n",
        "Accuracy alone never tells the full story.There are a number of other metrics borrowed from statistics that are commonly used on classification models.\n",
        "\n",
        "It's possible for a model to have a high accuracy, but score very low on some of the following metrics:\n",
        "\n",
        "* **True Positives:** The cases where we predicted positively, and the actual label was positive.\n",
        "\n",
        "* **True Negatives:** The cases where we predicted negatively, and the actual label was negative.\n",
        "\n",
        "* **False Positives:** The cases where we predicted positively, but the actual label was negative.\n",
        "\n",
        "* **False Negatives:** The cases where we predicted negatively, but the actual label was positive.\n",
        "\n",
        "* **False Positive Rate:** Corresponds to the proportion of negative datapoints incorrectly considered positive relative to all negative points.\n",
        ">$FPR = \\frac{FP}{TN + FP}$\n",
        "\n",
        "* **Sensitivity:** _(Also known as True Positive Rate)_ corresponds to the proportion of positive datapoints correctly considered as positive relative to all positive points.\n",
        ">$TPR = \\frac{TP}{TP + FN}$\n",
        "\n",
        "\n",
        "* **Specificity:** _(Also known as True Negative Rate)_ corresponds to the proportion of negative datapoints correctly considered negative relative to all negative points.\n",
        ">$TNR = \\frac{TN}{TN + FP}$\n",
        "\n",
        "* **Precision:** Proportion of correctly labeled positive points relative to the number of positive predictions\n",
        ">$Precision = \\frac{TP}{TP+FP}$\n",
        "\n",
        "* **Recall:** Proportion of correctly labeled positive points relative to all points that were actually positive.\n",
        ">$Recall = \\frac{TP}{TP+FN}$\n",
        "\n",
        "* **F1 score:** Measure of a balance between precision and recall.\n",
        ">$F1 = 2 \\cdot \\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ8DZPlhIl0L"
      },
      "source": [
        "#### 2.4.3) Tradeoffs Between Metrics\n",
        "\n",
        "Oftentimes, our definition of a \"good\" model varies by situation or application case. In some cases, we might prefer a different tradeoff between accuracy, false positive rate, and false negative rate.\n",
        "\n",
        "**2.4)** Read through the following scenarios. For each case, state which metrics you would prioritize, and why.\n",
        "\n",
        "Scenario 1:\n",
        "\n",
        ">Doctors have identified a new extremely rare, but also very deadly disease. Fortunately, they also discover a simple medication, that if taken early enough, can prevent the disease. The doctors plan to use a machine learning model to predict which of their patients are at high-risk for getting the disease. A positively labeled patient is high-risk, while a negatively labeled patient is low-risk.\n",
        "\n",
        "Scenario 2:\n",
        ">Data Is Cool Inc. is a company that attracts many highly (and equally) qualified applicants to its job posting. They are overwhelmed with the number of applications received, so the company implements a machine learning model to sort all the incoming resumes. A positively labeled resume gets passed to a recruiter for a very thorough, but time-costly review. Negatively labeled resumes are held for future job openings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xJQfVP2_xWc"
      },
      "source": [
        "## 3) Tying It All Together -- Choosing A Model to Deploy\n",
        "\n",
        "\n",
        "Now that we've seen many different evaluation metrics, let's put what we've learned into practice!\n",
        "\n",
        "One of the most common problems you'll encounter as a data scientist is to decide between a set of candidate models.\n",
        "\n",
        "Each of the following code boxes below generates a candidate classifier for predicting high vs low obesity rates in cities. The models can differ in different ways: number of features, learning algorithm used, number of datapoints, etc. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-pp0Hf9aYp8"
      },
      "source": [
        "# Classifier A\n",
        "x_A = df[[\"Count_Person\",\n",
        "          \"Median_Income_Person\"]]\n",
        "y_A = df[\"Label\"]\n",
        "\n",
        "classifierA = linear_model.Perceptron()\n",
        "classifierA.fit(x_A, y_A)\n",
        "scores = cross_val_score(classifierA, x_A, y_A, cv=5)\n",
        "\n",
        "print('Classifier A')\n",
        "print('-------------')\n",
        "print('Number of Data Points:', x_A.shape[0])\n",
        "print('Number of Features:', x_A.shape[1])\n",
        "print('Classification Accuracy: %.2f' % classifierA.score(x_A, y_A))\n",
        "print('5-Fold Cross Validation Accuracy: %.2f' % np.mean(scores))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuJ-EzxZdKVl"
      },
      "source": [
        "# Classifier A\n",
        "x_A = df[[\"Percent_Person_PhysicalInactivity\",\n",
        "          \"Median_Income_Person\"]]\n",
        "y_A = df[\"Label\"]\n",
        "\n",
        "classifierA = svm.SVC()\n",
        "classifierA.fit(x_A, y_A)\n",
        "scores = cross_val_score(classifierA, x_A, y_A, cv=5)\n",
        "\n",
        "print('Classifier A')\n",
        "print('-------------')\n",
        "print('Number of Data Points:', x_A.shape[0])\n",
        "print('Number of Features:', x_A.shape[1])\n",
        "print('Training Classification Accuracy: %.2f' % classifierA.score(x_A, y_A))\n",
        "print('5-Fold Cross Validation Accuracy: %.2f' % np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e04m-KpZeOcp"
      },
      "source": [
        "# Classifier B\n",
        "x_B = df.dropna()[[\n",
        "          \"Percent_NoHealthInsurance\",\n",
        "          \"Percent_Person_PhysicalInactivity\",\n",
        "          \"Percent_Person_SleepLessThan7Hours\",\n",
        "          \"Percent_Person_WithHighBloodPressure\",\n",
        "          \"Percent_Person_WithMentalHealthNotGood\",\n",
        "          \"Percent_Person_WithHighCholesterol\"\n",
        "]]\n",
        "y_B = df.dropna()[\"Label\"]\n",
        "\n",
        "classifierB = tree.DecisionTreeClassifier()\n",
        "classifierB.fit(x_B, y_B)\n",
        "scores = cross_val_score(classifierB, x_B, y_B, cv=5)\n",
        "\n",
        "print('Classifier B')\n",
        "print('-------------')\n",
        "print('Number of Data Points:', x_B.shape[0])\n",
        "print('Number of Features:', x_B.shape[1])\n",
        "print('Training Classification Accuracy: %.2f' % classifierB.score(x_B, y_B))\n",
        "print('5-Fold Cross Validation Accuracy: %.2f' % np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1AtT499foLS"
      },
      "source": [
        "# Classifier C\n",
        "x_C = df.dropna()[[\n",
        "          \"Percent_NoHealthInsurance\",\n",
        "          \"Percent_Person_PhysicalInactivity\",\n",
        "          \"Percent_Person_SleepLessThan7Hours\",\n",
        "          \"Percent_Person_WithHighBloodPressure\",\n",
        "          \"Percent_Person_WithMentalHealthNotGood\",\n",
        "          \"Percent_Person_WithHighCholesterol\"\n",
        "]]\n",
        "y_C = df.dropna()[\"Label\"]\n",
        "\n",
        "classifierC = linear_model.Perceptron()\n",
        "classifierC.fit(x_C, y_C)\n",
        "scores = cross_val_score(classifierC, x_C, y_C, cv=5)\n",
        "\n",
        "print('Classifier C')\n",
        "print('-------------')\n",
        "print('Number of Data Points:', x_C.shape[0])\n",
        "print('Number of Features:', x_C.shape[1])\n",
        "print('Training Classification Accuracy: %.2f' % classifierC.score(x_C, y_C))\n",
        "print('5-Fold Cross Validation Accuracy: %.2f' % np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u7xbAicaZR4"
      },
      "source": [
        "**3A)** Run the code boxes above and select which model you would choose to deploy. Justify your answer.\n",
        "\n",
        "**3B)** Consider a new Classifier D. Its results look like this:\n",
        ">Number of Data Points: 5,000 \\\n",
        ">Number of Features: 10,000 \\\n",
        ">Training Classification Accuracy: 98% \\\n",
        ">5-Fold Cross Validation Accuracy: 95%. \n",
        "\n",
        "Would you deploy classifier D? Name one advantage and one disadvantage of such a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxkyeNejly6y"
      },
      "source": [
        "## 4) Extension: What about YOUR city?\n",
        "\n",
        "Now that we've got a model trained up, let's play with it! \n",
        "\n",
        "1. Use the [Data Commons Place Explorer](https://datacommons.org/place) to find the DCID of a town or city local to you. \n",
        "\n",
        "2. Use the code box below to add your local town or city's data, and run the model that data.\n",
        "\n",
        "**Note: Data may not be available for all locations. If you encounter errors, please try a different location!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uB697BnhOiWN"
      },
      "source": [
        "your_local_dcid = \"geoId/0649670\" # Replace with your own!\n",
        "\n",
        "# Get your local data from data commons\n",
        "local_data = datacommons_pandas.build_multivariate_dataframe(your_local_dcid,stat_vars_to_query)\n",
        "\n",
        "# Cleaning and Preprocessing\n",
        "local_data['DCID'] = local_data.index\n",
        "city_name_dict = datacommons.get_property_values(city_dcids, 'name')\n",
        "city_name_dict = {key:value[0] for key, value in city_name_dict.items()}\n",
        "local_data['City'] = pd.Series(city_name_dict)\n",
        "local_data.set_index('City', inplace=True)\n",
        "local_data.rename(columns={\"dc/e9gftzl2hm8h9\":\"Commute_Time\"}, inplace=True)\n",
        "avg_commute_time = local_data[\"Commute_Time\"]/local_data[\"Count_Person\"]\n",
        "local_data[\"Commute_Time\"] = avg_commute_time\n",
        "percent_noHealthInsurance = local_data[\"Count_Person_NoHealthInsurance\"]/local_data[\"Count_Person\"]\n",
        "local_data[\"Percent_NoHealthInsurance\"] = percent_noHealthInsurance\n",
        "local_data[\"Label\"] = local_data['Percent_Person_Obesity'] >= 30.0\n",
        "local_data[\"Label\"] = local_data[\"Label\"].astype(int)\n",
        "\n",
        "# Build data to feed into model\n",
        "x_local = local_data[[\n",
        "              \"Median_Income_Person\",\n",
        "              \"Percent_NoHealthInsurance\",\n",
        "              \"Percent_Person_PhysicalInactivity\",\n",
        "              \"Percent_Person_SleepLessThan7Hours\",\n",
        "              \"Percent_Person_WithHighBloodPressure\",\n",
        "              \"Percent_Person_WithMentalHealthNotGood\",\n",
        "              \"Percent_Person_WithHighCholesterol\",\n",
        "              \"Commute_Time\"          \n",
        "]]\n",
        "x_local = scaler.transform(x_local)\n",
        "y_local = local_data[\"Label\"]\n",
        "\n",
        "\n",
        "# Make Prediction\n",
        "prediction = large_model.predict(x_local)\n",
        "\n",
        "# Report Results\n",
        "print(f'Prediction for {local_data.index[0]}:')\n",
        "print(f'\\tThe predicted label was {prediction[0]}')\n",
        "print(f'\\tThe actual label was {y_local[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71cqwFb8PFZv"
      },
      "source": [
        "**4A)** How well did the model do? Was it able to classify your city or town correctly?\n",
        "\n",
        "**4B)** Can you find a city or town that the model predicts incorrectly? Why do you think the model predicts it incorrectly?\n",
        "\n"
      ]
    }
  ]
}